{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Create Env",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGst78zHPfyS",
        "outputId": "ebb74356-b3cb-44a0-9c6c-1fa45b6dbb95"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/da/4ff439558641a26dd29b04c25947e6c0ace041f56b2aa2ef1134edab06b8/pygame-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 3.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVGRLL1fPoLr"
      },
      "source": [
        "import pygame\n",
        "import numpy as np\n",
        "from random import randint\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EFA4HCwq5U5"
      },
      "source": [
        "class Field:\n",
        "    def __init__(self, height=10, width=5):\n",
        "        self.width      = width\n",
        "        self.height     = height\n",
        "        self.body       = np.zeros(shape=(self.height, self.width))\n",
        "    \n",
        "    def update_field(self,walls, player):\n",
        "        try:\n",
        "            # Clear the field:\n",
        "            self.body = np.zeros(shape=(self.height, self.width))\n",
        "            # Put the walls on the field:\n",
        "            for wall in walls:\n",
        "                if not wall.out_of_range :\n",
        "                    self.body[wall.y:min(wall.y+wall.height,self.height),:] = wall.body\n",
        "\n",
        "            # Put the player on the field:\n",
        "            self.body[player.y:player.y+player.height,\n",
        "                      player.x:player.x+player.width] += player.body \n",
        "        except :\n",
        "            pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNW9y7WvrUAa"
      },
      "source": [
        "class Wall:        \n",
        "    def __init__(self, height = 5, width=100,  hole_width = 20, y = 0, speed = 1, field = None):\n",
        "        self.height       = height\n",
        "        self.width        = width\n",
        "        self.hole_width   = hole_width\n",
        "        self.y            = y\n",
        "        self.speed        = speed\n",
        "        self.field        = field\n",
        "        self.body_unit     = 1\n",
        "        self.body         = np.ones(shape = (self.height, self.width))*self.body_unit\n",
        "        self.out_of_range = False\n",
        "        self.create_hole()\n",
        "    def create_hole(self):\n",
        "        hole = np.zeros(shape = (self.height, self.hole_width))\n",
        "        hole_pos = randint(0,self.width-self.hole_width)\n",
        "        self.body[ : , hole_pos:hole_pos+self.hole_width] = 0\n",
        "    def move(self):\n",
        "        self.y += self.speed\n",
        "        self.out_of_range = True if ((self.y + self.height) > self.field.height) else False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhafWVEbreFh"
      },
      "source": [
        "class Player:\n",
        "    def __init__(self, height = 5, max_width = 10 , width=2, x = 0, y = 0, speed = 2):\n",
        "        self.height        = height\n",
        "        self.max_width     = max_width\n",
        "        self.width         = width\n",
        "        self.x             = x\n",
        "        self.y             = y\n",
        "        self.speed         = speed\n",
        "        self.body_unit     = 2\n",
        "        self.body          = np.ones(shape = (self.height, self.width))*self.body_unit\n",
        "        self.stamina       = 20\n",
        "        self.max_stamina   = 20\n",
        "    def move(self, field, direction = 0 ):\n",
        "        '''\n",
        "        Moves the player :\n",
        "         - No change          = 0\n",
        "         - left, if direction  = 1\n",
        "         - right, if direction = 2\n",
        "        '''\n",
        "        val2dir   = {0:0 , 1:-1 , 2:1}\n",
        "        direction = val2dir[direction]\n",
        "        next_x = (self.x + self.speed*direction)\n",
        "        if not (next_x + self.width > field.width or next_x < 0):\n",
        "            self.x += self.speed*direction\n",
        "            self.stamina -= 1 \n",
        "    def change_width(self, action = 0):\n",
        "        '''\n",
        "        Change the player's width:\n",
        "         - No change          = 0\n",
        "         - narrow by one unit = 3\n",
        "         - widen by one unit  = 4\n",
        "        '''\n",
        "        val2act   = {0:0 , 3:-1 , 4:1}\n",
        "        action = val2act[action]\n",
        "        new_width = self.width+action\n",
        "        player_end = self.x + new_width\n",
        "        if new_width <= self.max_width and new_width > 0 and player_end <= self.max_width:\n",
        "            self.width = new_width\n",
        "            self.body  = np.ones(shape = (self.height, self.width))*self.body_unit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AOdluVjsuEY"
      },
      "source": [
        "class Environment:\n",
        "    P_HEIGHT      = 2  # Height of the player\n",
        "    F_HEIGHT      = 20 # Height of the field\n",
        "    W_HEIGHT      = 2  # Height of the walls\n",
        "    WIDTH         = 10 # Width of the field and the walls\n",
        "    MIN_H_WIDTH   = 2  # Minimum width of the holes\n",
        "    MAX_H_WIDTH   = 6  # Maximum width of the holes\n",
        "    MIN_P_WIDTH   = 2  # Minimum Width of the player\n",
        "    MAX_P_WIDTH   = 6  # Maximum Width of the player\n",
        "    HEIGHT_MUL    = 30 # Height Multiplier (used to draw np.array as blocks in pygame )\n",
        "    WIDTH_MUL     = 40 # Width Multiplier (used to draw np.array as blocks in pygame )\n",
        "    WINDOW_HEIGHT = (F_HEIGHT+1) * HEIGHT_MUL # Height of the pygame window\n",
        "    WINDOW_WIDTH  = (WIDTH) * WIDTH_MUL       # Widh of the pygame window\n",
        "    \n",
        "    ENVIRONMENT_SHAPE = (F_HEIGHT,WIDTH,1)\n",
        "    ACTION_SPACE      = [0,1,2,3,4]\n",
        "    ACTION_SPACE_SIZE = len(ACTION_SPACE)\n",
        "    PUNISHMENT        = -100  # Punishment increment\n",
        "    REWARD            = 10    # Reward increment\n",
        "    score             = 0     # Initial Score\n",
        "    \n",
        "    MOVE_WALL_EVERY   = 4     # Every how many frames the wall moves.\n",
        "    MOVE_PLAYER_EVERY = 1     # Every how many frames the player moves.\n",
        "    frames_counter    = 0\n",
        "\n",
        "    def __init__(self):\n",
        "        # Colors:\n",
        "        self.BLACK      = (25,25,25)\n",
        "        self.WHITE      = (255,255,255)\n",
        "        self.RED        = (255, 80, 80)\n",
        "        self.BLUE       = (80, 80, 255)\n",
        "        self.field = self.walls = self.player = None\n",
        "        self.current_state = self.reset()\n",
        "        self.val2color  = {0:self.WHITE, self.walls[0].body_unit:self.BLACK, self.player.body_unit:self.BLACK, self.MAX_VAL:self.RED}\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.score          = 0\n",
        "        self.frames_counter = 0\n",
        "        self.game_over      = False\n",
        "        \n",
        "        self.field = Field(height=self.F_HEIGHT, width=self.WIDTH )\n",
        "\n",
        "        w1    = Wall( height = self.W_HEIGHT, width=self.WIDTH,\n",
        "                      hole_width = randint(self.MIN_H_WIDTH,self.MAX_H_WIDTH),\n",
        "                     field = self.field)\n",
        "        \n",
        "        self.walls = deque([w1])\n",
        "        p_width = randint(self.MIN_P_WIDTH,self.MAX_P_WIDTH)\n",
        "\n",
        "        self.player    = Player( height = self.P_HEIGHT, max_width = self.WIDTH,\n",
        "                                width = p_width,\n",
        "                                x = randint(0,self.field.width-p_width),\n",
        "                                y = int(self.field.height*0.7), speed = 1)\n",
        "        \n",
        "        self.MAX_VAL = self.player.body_unit + w1.body_unit\n",
        "\n",
        "        self.field.update_field(self.walls, self.player)\n",
        "        \n",
        "        observation = self.field.body/self.MAX_VAL\n",
        "        return observation\n",
        "\n",
        "    def print_text(self, WINDOW = None, text_cords = (0,0), center = False, text = \"\", color = (0,0,0), size = 32):\n",
        "\n",
        "        pygame.init()\n",
        "        font = pygame.font.Font('freesansbold.ttf', size) \n",
        "        text_to_print = font.render(text, True, color) \n",
        "        textRect = text_to_print.get_rect()\n",
        "        if center:\n",
        "            textRect.center = text_cords\n",
        "        else:\n",
        "            textRect.x = text_cords[0]\n",
        "            textRect.y = text_cords[1]\n",
        "        WINDOW.blit(text_to_print, textRect)\n",
        "        \n",
        "    def step(self, action):\n",
        "        global score_increased\n",
        "\n",
        "        self.frames_counter += 1\n",
        "        reward = 0\n",
        "\n",
        "        # If the performed action is (move) then player.move method is called:\n",
        "        if action in [1,2]:\n",
        "            self.player.move(direction = action, field = self.field)\n",
        "\n",
        "        # If the performed action is (change_width) then player.change_width method is called:\n",
        "        elif action in [3,4]:\n",
        "            self.player.change_width(action = action)                \n",
        "        \n",
        "        # Move the wall one step (one step every WALL_SPEED frames):\n",
        "        if self.frames_counter % self.WALL_SPEED == 0:\n",
        "            # move the wall one step\n",
        "            self.walls[-1].move()\n",
        "            # reset the frames counter\n",
        "            self.frames_counter = 0\n",
        "        \n",
        "        # Update the field :\n",
        "        self.field.update_field(self.walls, self.player)\n",
        "\n",
        "        # If the player passed a wall successfully increase the reward +1\n",
        "        if ((self.walls[-1].y) == (self.player.y + self.player.height)) and not score_increased :\n",
        "            reward += self.REWARD\n",
        "            self.score  += self.REWARD\n",
        "            \n",
        "            # Increase player's stamina every time it passed a wall successfully  \n",
        "            self.player.stamina = min(self.player.max_stamina, self.player.stamina+10)\n",
        "            # score_increased : a flag to make sure that reward increases once per wall \n",
        "            score_increased = True\n",
        "            \n",
        "        \n",
        "        #  Lose Conditions : \n",
        "        # C1 : The player hits a wall\n",
        "        # C2 : Player's width was far thinner than hole's width\n",
        "        # C3 : Player fully consumed its stamina (energy)\n",
        "        lose_conds = [self.MAX_VAL in self.field.body,\n",
        "                      ((self.player.y == self.walls[-1].y) and (self.player.width < (self.walls[-1].hole_width-1))),\n",
        "                      self.player.stamina <=0]\n",
        "        \n",
        "\n",
        "        # If one lose condition or more happend, the game ends:\n",
        "        if True in lose_conds:\n",
        "            self.game_over = True\n",
        "            reward = self.PUNISHMENT\n",
        "            return self.field.body/self.MAX_VAL, reward, self.game_over\n",
        "\n",
        "        # Check if a wall moved out of the scene:\n",
        "        if self.walls[-1].out_of_range:\n",
        "            # Create a new wall\n",
        "            self.walls[-1] = Wall( height = self.W_HEIGHT, width = self.WIDTH,\n",
        "                                   hole_width = randint(self.MIN_H_WIDTH,self.MAX_H_WIDTH),\n",
        "                                   field = self.field)\n",
        "\n",
        "            score_increased = False\n",
        "\n",
        "        \n",
        "        # Return New Observation , reward, game_over(bool)\n",
        "        return self.field.body/self.MAX_VAL, reward, self.game_over\n",
        "    \n",
        "    def render(self, WINDOW = None, human=False):\n",
        "        if human:\n",
        "            ################ Check Actions #####################\n",
        "            action = 0\n",
        "            events = pygame.event.get()\n",
        "            for event in events:\n",
        "                if event.type == pygame.QUIT:\n",
        "                    self.game_over = True\n",
        "                if event.type == pygame.KEYDOWN:\n",
        "                    if event.key == pygame.K_LEFT:\n",
        "                        action = 1\n",
        "                    if event.key == pygame.K_RIGHT:\n",
        "                        action = 2\n",
        "\n",
        "                    if event.key == pygame.K_UP:\n",
        "                        action = 4\n",
        "                    if event.key == pygame.K_DOWN:\n",
        "                        action = 3\n",
        "            ################## Step ############################            \n",
        "            _,reward, self.game_over = self.step(action)\n",
        "        ################ Draw Environment ###################\n",
        "        WINDOW.fill(self.WHITE)\n",
        "        self.field.update_field(self.walls, self.player)\n",
        "        for r in range(self.field.body.shape[0]):\n",
        "            for c in range(self.field.body.shape[1]):\n",
        "                pygame.draw.rect(WINDOW,\n",
        "                                 self.val2color[self.field.body[r][c]],\n",
        "                                 (c*self.WIDTH_MUL, r*self.HEIGHT_MUL, self.WIDTH_MUL, self.HEIGHT_MUL))\n",
        "\n",
        "        self.print_text(WINDOW = WINDOW, text_cords = (self.WINDOW_WIDTH // 2, int(self.WINDOW_HEIGHT*0.1)),\n",
        "                       text = str(self.score), color = self.RED, center = True)\n",
        "        self.print_text(WINDOW = WINDOW, text_cords = (0, int(self.WINDOW_HEIGHT*0.9)),\n",
        "                       text = str(self.player.stamina), color = self.RED)\n",
        "        \n",
        "        pygame.display.update()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUH2efQ21AoL"
      },
      "source": [
        "# Agent class\n",
        "class DQNAgent:\n",
        "    def __init__(self, name, env, conv_list, dense_list, util_list):\n",
        "        self.env = env\n",
        "        self.conv_list  = conv_list\n",
        "        self.dense_list = dense_list\n",
        "        self.name = [str(name) +\" | \" + \"\".join(str(c)+\"C | \" for c in conv_list) + \"\".join(str(d) + \"D | \" for d in dense_list) + \"\".join(u + \" | \" for u in util_list) ][0]\n",
        "        \n",
        "        # Main model\n",
        "        self.model = self.create_model(self.conv_list, self.dense_list)\n",
        "\n",
        "        # Target network\n",
        "        self.target_model = self.create_model(self.conv_list, self.dense_list)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # An array with last n steps for training\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "\n",
        "        # Custom tensorboard object\n",
        "        self.tensorboard = ModifiedTensorBoard(name, log_dir=\"{}logs/{}-{}\".format(PATH, name, int(time.time())))\n",
        "    \n",
        "        # Used to count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "        \n",
        "    # Creates a convolutional block given (filters) number of filters, (dropout) dropout rate, \n",
        "    # (bn) a boolean variable indecating the use of BatchNormalization,\n",
        "    # (pool) a boolean variable indecating the use of MaxPooling2D \n",
        "    def conv_block(self, inp, filters=64, bn=True, pool=True, dropout = 0.2):\n",
        "        _ = Conv2D(filters=filters, kernel_size=3, activation='relu')(inp)\n",
        "        if bn:\n",
        "            _ = BatchNormalization()(_)\n",
        "        if pool:\n",
        "            _ = MaxPooling2D(pool_size=(2, 2))(_)\n",
        "        if dropout > 0:\n",
        "            _ = Dropout(0.2)(_)\n",
        "        return _\n",
        "    # Creates the model with the given specifications:\n",
        "    def create_model(self, conv_list, dense_list):\n",
        "        # Defines the input layer with shape = ENVIRONMENT_SHAPE\n",
        "        input_layer = Input(shape=self.env.ENVIRONMENT_SHAPE)\n",
        "        # Defines the first convolutional block:\n",
        "        _ = self.conv_block(input_layer, filters=conv_list[0], bn=False, pool=False)\n",
        "        # If number of convolutional layers is 2 or more, use a loop to create them.\n",
        "        if len(conv_list)>1:\n",
        "            for c in conv_list[1:]:\n",
        "                _ = self.conv_block(_, filters=c)\n",
        "        # Flatten the output of the last convolutional layer.\n",
        "        _  = Flatten()(_)\n",
        "\n",
        "        # Creating the dense layers:\n",
        "        for d in dense_list:\n",
        "            _ = Dense(units=d, activation='relu')(_)\n",
        "        # The output layer has 5 nodes (one node per action)\n",
        "        output = Dense(units=self.env.ACTION_SPACE_SIZE,\n",
        "                          activation='linear', name='output')(_)\n",
        "\n",
        "        # Put it all together:\n",
        "        model = Model(inputs=input_layer, outputs=[output])\n",
        "        model.compile(optimizer=Adam(lr=0.001),\n",
        "                      loss={'output': 'mse'},\n",
        "                      metrics={'output': 'accuracy'})\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Adds step's data to a memory replay array\n",
        "    # (observation space, action, reward, new observation space, done)\n",
        "    def update_replay_memory(self, transition):\n",
        "        self.replay_memory.append(transition)\n",
        "\n",
        "    # Trains main network every step during episode\n",
        "    def train(self, terminal_state, step):\n",
        "        # Start training only if certain number of samples is already saved\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        # Get a minibatch of random samples from memory replay table\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
        "\n",
        "        # Get current states from minibatch, then query NN model for Q values\n",
        "        current_states = np.array([transition[0] for transition in minibatch])\n",
        "        current_qs_list = self.model.predict(current_states.reshape(-1, *env.ENVIRONMENT_SHAPE))\n",
        "        \n",
        "\n",
        "        # Get future states from minibatch, then query NN model for Q values\n",
        "        # When using target network, query it, otherwise main network should be queried\n",
        "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
        "        future_qs_list = self.target_model.predict(new_current_states.reshape(-1, *env.ENVIRONMENT_SHAPE))\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        # Now we need to enumerate our batches\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "\n",
        "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
        "            # almost like with Q Learning, but we use just part of equation here\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = reward\n",
        "\n",
        "            # Update Q value for given state\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # And append to our training data\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "            \n",
        "        # Fit on all samples as one batch, log only on terminal state\n",
        "        self.model.fit(x = np.array(X).reshape(-1, *env.ENVIRONMENT_SHAPE),\n",
        "                       y = np.array(y),\n",
        "                       batch_size = MINIBATCH_SIZE, verbose = 0,\n",
        "                       shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
        "\n",
        "        # Update target network counter every episode\n",
        "        if terminal_state:\n",
        "            self.target_update_counter += 1\n",
        "\n",
        "        # If counter reaches set value, update target network with weights of main network\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "    # Queries main network for Q values given current observation space (environment state)\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(state.reshape(-1, *env.ENVIRONMENT_SHAPE))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}